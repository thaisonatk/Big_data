{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "1b356711-5add-4638-ab51-68002a59d115"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not downloading, Hadoop folder hadoop-3.4.0 already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "9f7d3501-6c63-444a-997b-a604b9df9bf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "5c181645-3b6f-4a4e-ed8f-f827e66cee36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Thai Son\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "b32df533-9291-4565-9177-febf486df6b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 18:22:10,163 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 18:22:12,352 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 18:22:12,547 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 18:22:12,547 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 18:22:12,571 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:12,912 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 18:22:12,941 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 18:22:13,318 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2033650442_0001\n",
            "2024-04-23 18:22:13,318 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 18:22:13,570 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 18:22:13,572 INFO mapreduce.Job: Running job: job_local2033650442_0001\n",
            "2024-04-23 18:22:13,582 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 18:22:13,585 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 18:22:13,595 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:13,600 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:13,670 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 18:22:13,676 INFO mapred.LocalJobRunner: Starting task: attempt_local2033650442_0001_m_000000_0\n",
            "2024-04-23 18:22:13,717 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:13,717 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:13,756 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:13,768 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:13,808 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 18:22:13,901 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 18:22:13,902 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 18:22:13,902 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 18:22:13,902 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 18:22:13,902 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 18:22:13,908 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 18:22:13,911 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 18:22:13,919 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 18:22:13,926 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 18:22:13,930 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 18:22:13,930 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 18:22:13,931 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 18:22:13,931 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 18:22:13,935 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 18:22:13,935 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 18:22:13,935 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 18:22:13,936 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 18:22:13,937 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 18:22:13,938 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 18:22:13,959 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 18:22:13,966 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 18:22:13,968 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 18:22:13,968 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 18:22:13,972 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:13,972 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 18:22:13,972 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 18:22:13,972 INFO mapred.MapTask: bufstart = 0; bufend = 10; bufvoid = 104857600\n",
            "2024-04-23 18:22:13,972 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 18:22:13,983 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 18:22:14,000 INFO mapred.Task: Task:attempt_local2033650442_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:14,003 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 18:22:14,004 INFO mapred.Task: Task 'attempt_local2033650442_0001_m_000000_0' done.\n",
            "2024-04-23 18:22:14,012 INFO mapred.Task: Final Counters for attempt_local2033650442_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=857630\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=10\n",
            "\t\tMap output materialized bytes=18\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=21\n",
            "\t\tTotal committed heap usage (bytes)=367001600\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "2024-04-23 18:22:14,012 INFO mapred.LocalJobRunner: Finishing task: attempt_local2033650442_0001_m_000000_0\n",
            "2024-04-23 18:22:14,013 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 18:22:14,018 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 18:22:14,023 INFO mapred.LocalJobRunner: Starting task: attempt_local2033650442_0001_r_000000_0\n",
            "2024-04-23 18:22:14,038 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:14,038 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:14,039 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:14,046 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@46304ca2\n",
            "2024-04-23 18:22:14,048 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:14,080 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 18:22:14,087 INFO reduce.EventFetcher: attempt_local2033650442_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 18:22:14,139 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2033650442_0001_m_000000_0 decomp: 14 len: 18 to MEMORY\n",
            "2024-04-23 18:22:14,147 INFO reduce.InMemoryMapOutput: Read 14 bytes from map-output for attempt_local2033650442_0001_m_000000_0\n",
            "2024-04-23 18:22:14,151 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14\n",
            "2024-04-23 18:22:14,156 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 18:22:14,157 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:14,158 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 18:22:14,167 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 18:22:14,167 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 18:22:14,169 INFO reduce.MergeManagerImpl: Merged 1 segments, 14 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 18:22:14,172 INFO reduce.MergeManagerImpl: Merging 1 files, 18 bytes from disk\n",
            "2024-04-23 18:22:14,173 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 18:22:14,173 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 18:22:14,174 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 18:22:14,175 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:14,191 INFO mapred.Task: Task:attempt_local2033650442_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:14,195 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:14,195 INFO mapred.Task: Task attempt_local2033650442_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 18:22:14,197 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2033650442_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 18:22:14,198 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 18:22:14,199 INFO mapred.Task: Task 'attempt_local2033650442_0001_r_000000_0' done.\n",
            "2024-04-23 18:22:14,199 INFO mapred.Task: Final Counters for attempt_local2033650442_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141977\n",
            "\t\tFILE: Number of bytes written=857670\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=18\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=367001600\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=22\n",
            "2024-04-23 18:22:14,200 INFO mapred.LocalJobRunner: Finishing task: attempt_local2033650442_0001_r_000000_0\n",
            "2024-04-23 18:22:14,200 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 18:22:14,579 INFO mapreduce.Job: Job job_local2033650442_0001 running in uber mode : false\n",
            "2024-04-23 18:22:14,580 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 18:22:14,582 INFO mapreduce.Job: Job job_local2033650442_0001 completed successfully\n",
            "2024-04-23 18:22:14,592 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283886\n",
            "\t\tFILE: Number of bytes written=1715300\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=10\n",
            "\t\tMap output materialized bytes=18\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=18\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=21\n",
            "\t\tTotal committed heap usage (bytes)=734003200\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=22\n",
            "2024-04-23 18:22:14,592 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "670377d7-8938-44d1-a7f7-4e4cd56f92d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "392bd52b-8392-4251-b3d3-bba9abd3b00b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-23 18:22 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         10 2024-04-23 18:22 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "76dd150e-b9b9-4dc4-e095-30483a0aff16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 10 Apr 23 18:22 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 23 18:22 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "de263bc3-6d6c-4545-813e-59d21a86751d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thai Son\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "6aab35c6-50f4-4c95-ce21-52dbf8868ab2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 18:22:21,521 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "ba7d4186-f7b8-4979-a405-64f312f03fc3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 18:22:23,624 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 18:22:25,916 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 18:22:26,129 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 18:22:26,129 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 18:22:26,156 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:26,460 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 18:22:26,490 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 18:22:26,826 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1315935306_0001\n",
            "2024-04-23 18:22:26,826 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 18:22:27,114 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 18:22:27,116 INFO mapreduce.Job: Running job: job_local1315935306_0001\n",
            "2024-04-23 18:22:27,125 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 18:22:27,128 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 18:22:27,137 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:27,141 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:27,199 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 18:22:27,206 INFO mapred.LocalJobRunner: Starting task: attempt_local1315935306_0001_m_000000_0\n",
            "2024-04-23 18:22:27,243 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:27,247 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:27,283 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:27,297 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:27,319 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 18:22:27,408 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 18:22:27,408 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 18:22:27,408 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 18:22:27,408 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 18:22:27,408 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 18:22:27,413 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 18:22:27,421 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:27,421 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 18:22:27,421 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 18:22:27,421 INFO mapred.MapTask: bufstart = 0; bufend = 17; bufvoid = 104857600\n",
            "2024-04-23 18:22:27,421 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 18:22:27,439 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 18:22:27,462 INFO mapred.Task: Task:attempt_local1315935306_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:27,465 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:27,465 INFO mapred.Task: Task 'attempt_local1315935306_0001_m_000000_0' done.\n",
            "2024-04-23 18:22:27,477 INFO mapred.Task: Final Counters for attempt_local1315935306_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=855520\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=17\n",
            "\t\tMap output materialized bytes=25\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "2024-04-23 18:22:27,478 INFO mapred.LocalJobRunner: Finishing task: attempt_local1315935306_0001_m_000000_0\n",
            "2024-04-23 18:22:27,480 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 18:22:27,488 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 18:22:27,492 INFO mapred.LocalJobRunner: Starting task: attempt_local1315935306_0001_r_000000_0\n",
            "2024-04-23 18:22:27,508 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:27,529 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:27,530 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:27,536 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a4ccbc6\n",
            "2024-04-23 18:22:27,539 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:27,562 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 18:22:27,577 INFO reduce.EventFetcher: attempt_local1315935306_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 18:22:27,618 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1315935306_0001_m_000000_0 decomp: 21 len: 25 to MEMORY\n",
            "2024-04-23 18:22:27,630 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local1315935306_0001_m_000000_0\n",
            "2024-04-23 18:22:27,636 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->21\n",
            "2024-04-23 18:22:27,642 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 18:22:27,645 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:27,645 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 18:22:27,654 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 18:22:27,655 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11 bytes\n",
            "2024-04-23 18:22:27,657 INFO reduce.MergeManagerImpl: Merged 1 segments, 21 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 18:22:27,658 INFO reduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk\n",
            "2024-04-23 18:22:27,659 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 18:22:27,660 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 18:22:27,664 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11 bytes\n",
            "2024-04-23 18:22:27,664 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:27,684 INFO mapred.Task: Task:attempt_local1315935306_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:27,688 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 18:22:27,690 INFO mapred.Task: Task attempt_local1315935306_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 18:22:27,693 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1315935306_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 18:22:27,694 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 18:22:27,694 INFO mapred.Task: Task 'attempt_local1315935306_0001_r_000000_0' done.\n",
            "2024-04-23 18:22:27,695 INFO mapred.Task: Final Counters for attempt_local1315935306_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141991\n",
            "\t\tFILE: Number of bytes written=855568\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=25\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=23\n",
            "2024-04-23 18:22:27,695 INFO mapred.LocalJobRunner: Finishing task: attempt_local1315935306_0001_r_000000_0\n",
            "2024-04-23 18:22:27,696 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 18:22:28,122 INFO mapreduce.Job: Job job_local1315935306_0001 running in uber mode : false\n",
            "2024-04-23 18:22:28,123 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 18:22:28,125 INFO mapreduce.Job: Job job_local1315935306_0001 completed successfully\n",
            "2024-04-23 18:22:28,141 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283900\n",
            "\t\tFILE: Number of bytes written=1711088\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=17\n",
            "\t\tMap output materialized bytes=25\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=25\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=727711744\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=23\n",
            "2024-04-23 18:22:28,142 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "315ce1ca-5187-4617-81b3-4a08f69c7946"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "059e4555-56c8-4209-d58f-33b2f7322aa2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tThai Son\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "cbd6b6be-edad-4cd5-de21-9ce3baf36466"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 18:22:33,195 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 18:22:35,426 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 18:22:35,588 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 18:22:35,588 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 18:22:35,617 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:35,906 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 18:22:35,944 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 18:22:36,305 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2126694950_0001\n",
            "2024-04-23 18:22:36,305 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 18:22:36,548 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 18:22:36,550 INFO mapreduce.Job: Running job: job_local2126694950_0001\n",
            "2024-04-23 18:22:36,562 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 18:22:36,565 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 18:22:36,574 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:36,575 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:36,645 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 18:22:36,658 INFO mapred.LocalJobRunner: Starting task: attempt_local2126694950_0001_m_000000_0\n",
            "2024-04-23 18:22:36,700 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:36,703 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:36,744 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:36,757 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:36,806 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 18:22:36,829 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:36,841 INFO mapred.Task: Task:attempt_local2126694950_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:36,843 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:36,844 INFO mapred.Task: Task attempt_local2126694950_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 18:22:36,851 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2126694950_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 18:22:36,852 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:36,853 INFO mapred.Task: Task 'attempt_local2126694950_0001_m_000000_0' done.\n",
            "2024-04-23 18:22:36,860 INFO mapred.Task: Final Counters for attempt_local2126694950_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=855484\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=29\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=23\n",
            "2024-04-23 18:22:36,860 INFO mapred.LocalJobRunner: Finishing task: attempt_local2126694950_0001_m_000000_0\n",
            "2024-04-23 18:22:36,861 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 18:22:37,563 INFO mapreduce.Job: Job job_local2126694950_0001 running in uber mode : false\n",
            "2024-04-23 18:22:37,566 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 18:22:37,570 INFO mapreduce.Job: Job job_local2126694950_0001 completed successfully\n",
            "2024-04-23 18:22:37,581 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=855484\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=29\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=23\n",
            "2024-04-23 18:22:37,581 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "a9026740-bdfa-44e7-d2d8-493943f3afdb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tThai Son\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "8a23a9d0-ba73-4982-c432-612ddc0d5244"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 18:22:41,577 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 18:22:45,070 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 18:22:45,268 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 18:22:45,269 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 18:22:45,295 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 18:22:45,575 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 18:22:45,601 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 18:22:45,930 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local239709510_0001\n",
            "2024-04-23 18:22:45,930 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 18:22:46,264 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 18:22:46,266 INFO mapreduce.Job: Running job: job_local239709510_0001\n",
            "2024-04-23 18:22:46,280 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 18:22:46,283 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 18:22:46,294 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:46,294 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:46,364 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 18:22:46,370 INFO mapred.LocalJobRunner: Starting task: attempt_local239709510_0001_m_000000_0\n",
            "2024-04-23 18:22:46,408 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 18:22:46,411 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 18:22:46,449 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 18:22:46,462 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+9\n",
            "2024-04-23 18:22:46,479 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 18:22:46,493 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 18:22:46,499 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 18:22:46,501 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 18:22:46,502 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 18:22:46,502 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 18:22:46,503 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 18:22:46,503 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 18:22:46,504 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 18:22:46,505 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 18:22:46,505 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 18:22:46,505 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 18:22:46,506 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 18:22:46,507 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 18:22:46,527 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 18:22:46,537 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 18:22:46,538 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 18:22:46,538 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 18:22:46,549 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:46,560 INFO mapred.Task: Task:attempt_local239709510_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 18:22:46,562 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 18:22:46,562 INFO mapred.Task: Task attempt_local239709510_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 18:22:46,564 INFO output.FileOutputCommitter: Saved output of task 'attempt_local239709510_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 18:22:46,570 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 18:22:46,570 INFO mapred.Task: Task 'attempt_local239709510_0001_m_000000_0' done.\n",
            "2024-04-23 18:22:46,580 INFO mapred.Task: Final Counters for attempt_local239709510_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=854996\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=375390208\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=22\n",
            "2024-04-23 18:22:46,580 INFO mapred.LocalJobRunner: Finishing task: attempt_local239709510_0001_m_000000_0\n",
            "2024-04-23 18:22:46,581 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 18:22:47,280 INFO mapreduce.Job: Job job_local239709510_0001 running in uber mode : false\n",
            "2024-04-23 18:22:47,283 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 18:22:47,286 INFO mapreduce.Job: Job job_local239709510_0001 completed successfully\n",
            "2024-04-23 18:22:47,299 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141909\n",
            "\t\tFILE: Number of bytes written=854996\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=375390208\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=22\n",
            "2024-04-23 18:22:47,300 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "98b2fd14-927b-4cf6-d812-3b4bb95fa179"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thai Son\t\n"
          ]
        }
      ]
    }
  ]
}